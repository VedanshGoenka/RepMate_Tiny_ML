{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model TFLite\n",
    "\n",
    "This is the test notebook for the inital model that we will be using for the project.\n",
    "There will be a lot of changes to this model as we go on, and we will be using this notebook to test those changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mac Environment Setup:\n",
    "\n",
    "1. Create a new virtual environment with Python 3.10. Do not use the setup_env.sh script.\n",
    "\n",
    "- `python3.10 -m venv .venv`\n",
    "- `source .venv/bin/activate`\n",
    "- `pip install -r requirements.txt`\n",
    "\n",
    "2. Verify that TensorFlow is working with the GPU.\n",
    "\n",
    "- Check the output of the following cell. You should see something like:\n",
    "  - TensorFlow version: 2.16.2\n",
    "  - Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "  - Result: tf.Tensor([5. 7. 9.], shape=(3,), dtype=float32)\n",
    "  - Metal device set to: Apple M1 Pro\n",
    "  - systemMemory: 16.00 GB\n",
    "  - maxCacheSize: 5.33 GB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Model TFLite\n",
    "\n",
    "## Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Video\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import pydot\n",
    "import graphviz\n",
    "import datetime\n",
    "\n",
    "# Verify TensorFlow installation\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Enable verbose logging\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n",
    "\n",
    "# Check GPU devices\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"Available GPUs:\", gpus)\n",
    "\n",
    "# Perform a computation\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    a = tf.constant([1.0, 2.0, 3.0])\n",
    "    b = tf.constant([4.0, 5.0, 6.0])\n",
    "    c = a + b\n",
    "    print(\"Result:\", c)\n",
    "\n",
    "\n",
    "# The first time this cell is run, make sure that you see:\n",
    "# Metal device set to: Apple M[GPU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories and filenames\n",
    "NOTEBOOK_TAG = \"initial_model\"\n",
    "MODEL_NAME = \"rep_mate.keras\"\n",
    "\n",
    "MODELS_DIR = NOTEBOOK_TAG + \"/models\"\n",
    "DATASET_DIR = NOTEBOOK_TAG + \"/dataset\"\n",
    "CHKPT_DIR = NOTEBOOK_TAG + \"/checkpoints\"\n",
    "LOG_DIR = NOTEBOOK_TAG + \"/logs\"\n",
    "PLOT_DIR = \"plots\"\n",
    "ANIM_DIR = \"anim\"\n",
    "\n",
    "# Create base directories with parents\n",
    "for directory in [MODELS_DIR, DATASET_DIR, CHKPT_DIR, PLOT_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Define model filenames\n",
    "SAVED_MODEL_FILENAME = os.path.join(MODELS_DIR, MODEL_NAME)\n",
    "FLOAT_TFL_MODEL_FILENAME = os.path.join(MODELS_DIR, MODEL_NAME + \"_float.tfl\")\n",
    "QUANTIZED_TFL_MODEL_FILENAME = os.path.join(MODELS_DIR, MODEL_NAME + \".tfl\")\n",
    "TFL_CC_MODEL_FILENAME = os.path.join(MODELS_DIR, MODEL_NAME + \".cc\")\n",
    "\n",
    "# Define dataset directories\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_DIR, \"validation\")\n",
    "TEST_DIR = os.path.join(DATASET_DIR, \"test\")\n",
    "\n",
    "# Create dataset subdirectories\n",
    "for directory in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAR_DATA = True\n",
    "\n",
    "# Directories to clean up\n",
    "DIRS = [\n",
    "    MODELS_DIR,\n",
    "    DATASET_DIR,\n",
    "    CHKPT_DIR,\n",
    "    TRAIN_DIR,\n",
    "    VAL_DIR,\n",
    "    TEST_DIR,\n",
    "    LOG_DIR,\n",
    "    CHKPT_DIR,\n",
    "]\n",
    "\n",
    "\n",
    "def remove_files_in_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        return\n",
    "\n",
    "    for root, dirs, files in os.walk(directory, topdown=False):\n",
    "        for name in files:\n",
    "            file_path = os.path.join(root, name)\n",
    "            try:\n",
    "                os.unlink(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "\n",
    "def cleanup_all_files(dirs=DIRS):\n",
    "    print(\n",
    "        \"WARNING: This will delete all files in the following directories and their subdirectories:\"\n",
    "    )\n",
    "    for dir in dirs:\n",
    "        if os.path.exists(dir):\n",
    "            total_files = sum([len(files) for _, _, files in os.walk(dir)])\n",
    "            print(f\"  - {dir} ({total_files} files)\")\n",
    "\n",
    "    confirmation = input(\"\\nType 'YES' to confirm deletion: \")\n",
    "\n",
    "    if confirmation == \"YES\":\n",
    "        for directory in dirs:\n",
    "            if os.path.exists(directory):\n",
    "                remove_files_in_directory(directory)\n",
    "                print(f\"Removed files from: {directory}\")\n",
    "        print(\"\\nAll files have been removed while preserving directory structure.\")\n",
    "    else:\n",
    "        print(\"\\nOperation cancelled.\")\n",
    "\n",
    "\n",
    "if CLEAR_DATA:\n",
    "    cleanup_all_files(DIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all files from data into notebook dataset directory\n",
    "!cp -r ../data/* {DATASET_DIR}\n",
    "\n",
    "# Get all the files in the dataset directory\n",
    "DATASET_FILES = glob.glob(os.path.join(DATASET_DIR, \"*\", \"*\", \"*\"))\n",
    "\n",
    "# Print the number of files in the dataset\n",
    "print(f\"Number of files in dataset: {len(DATASET_FILES)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python list to hold the dataset\n",
    "DATASET = []\n",
    "\n",
    "# Read each file and append to the dataset, add filename to the file data\n",
    "for f in DATASET_FILES:\n",
    "    with open(f, \"r\") as file:\n",
    "        f_c = file.read()\n",
    "    f_d = json.loads(f_c)\n",
    "    f_d[\"filename\"] = f\n",
    "    DATASET.append(f_d)\n",
    "\n",
    "print(f\"Number of files in dataset: {len(DATASET)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_matplotlib_backend(backend=\"inline\"):\n",
    "    \"\"\"\n",
    "    Switch matplotlib backend between 'inline' and 'notebook'\n",
    "    Args:\n",
    "        backend (str): Either 'inline' (static plots) or 'notebook' (interactive/animations)\n",
    "    \"\"\"\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    ipython.run_line_magic(\"matplotlib\", backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIFT_NAMES = {\"dC\": \"Dumbbell Curl\", \"bP\": \"Bench Press\", \"dF\": \"Dumbbell Fly\"}\n",
    "LIFT_CLASSES = {\n",
    "    \"p_f\": \"Perfect Form\",\n",
    "    \"l_i\": \"Lift Instability\",\n",
    "    \"p_m\": \"Partial Motion\",\n",
    "    \"o_a\": \"Off-Axis\",\n",
    "    \"s_w\": \"Swinging Weight\",\n",
    "    \"n_l\": \"No Lift\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lists(data):\n",
    "    time, aX, aY, aZ, gX, gY, gZ = [], [], [], [], [], [], []\n",
    "    for v in data:\n",
    "        time.append(v[\"t\"])\n",
    "        aX.append(v[\"aX\"])\n",
    "        aY.append(v[\"aY\"])\n",
    "        aZ.append(v[\"aZ\"])\n",
    "        gX.append(v[\"gX\"])\n",
    "        gY.append(v[\"gY\"])\n",
    "        gZ.append(v[\"gZ\"])\n",
    "    return time, aX, aY, aZ, gX, gY, gZ\n",
    "\n",
    "\n",
    "def plot_flat_dataset(data, show=True, save=False):\n",
    "    set_matplotlib_backend(\"inline\")\n",
    "    d = data[\"tSD\"]\n",
    "    lN = data[\"lN\"]\n",
    "    lC = data[\"lC\"]\n",
    "    time, aX, aY, aZ, gX, gY, gZ = create_lists(d)\n",
    "\n",
    "    # Create subplots for acceleration and gyroscope data\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "    # Acceleration subplot\n",
    "    axs[0].plot(time, aX, label=\"Ax\")\n",
    "    axs[0].plot(time, aY, label=\"Ay\")\n",
    "    axs[0].plot(time, aZ, label=\"Az\")\n",
    "    axs[0].set_title(\"3D Acceleration Over Time\")\n",
    "    axs[0].set_ylabel(\"Acceleration (m/s^2)\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Gyroscope subplot\n",
    "    axs[1].plot(time, gX, label=\"Gx\")\n",
    "    axs[1].plot(time, gY, label=\"Gy\")\n",
    "    axs[1].plot(time, gZ, label=\"Gz\")\n",
    "    axs[1].set_title(\"3D Gyroscope Over Time\")\n",
    "    axs[1].set_xlabel(\"Time (s)\")\n",
    "    axs[1].set_ylabel(\"Gyroscope (rad/s)\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Set Chart Title\n",
    "    fig.suptitle(f\"{LIFT_NAMES[lN]} - {LIFT_CLASSES[lC]}\", fontsize=16)\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    if save:\n",
    "        fig.savefig(f\"{PLOT_DIR}/{LIFT_NAMES[lN]}_{LIFT_CLASSES[lC]}_flat.png\")\n",
    "\n",
    "\n",
    "def plot_3d_trajectory(data, show=True, save=False):\n",
    "    set_matplotlib_backend(\"inline\")\n",
    "    d = data[\"tSD\"]\n",
    "    lN = data[\"lN\"]\n",
    "    lC = data[\"lC\"]\n",
    "    time, aX, aY, aZ, gX, gY, gZ = create_lists(d)\n",
    "\n",
    "    # Normalize time for color gradient\n",
    "    norm_time = (np.array(time) - min(time)) / (max(time) - min(time))\n",
    "\n",
    "    # Create 3D scatter plot for acceleration\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(121, projection=\"3d\")\n",
    "    scatter = ax.scatter(aX, aY, aZ, c=norm_time, cmap=\"viridis\", s=10)\n",
    "    ax.plot(aX, aY, aZ, color=\"gray\", alpha=0.5)  # Optional trajectory line\n",
    "    ax.set_title(\"3D Acceleration Trajectory\")\n",
    "    ax.set_xlabel(\"Ax (m/s^2)\")\n",
    "    ax.set_ylabel(\"Ay (m/s^2)\")\n",
    "    ax.set_zlabel(\"Az (m/s^2)\")\n",
    "    fig.colorbar(scatter, ax=ax, label=\"Time Gradient\")\n",
    "\n",
    "    # Create 3D scatter plot for gyroscope\n",
    "    ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "    scatter2 = ax2.scatter(gX, gY, gZ, c=norm_time, cmap=\"plasma\", s=10)\n",
    "    ax2.plot(gX, gY, gZ, color=\"gray\", alpha=0.5)  # Optional trajectory line\n",
    "    ax2.set_title(\"3D Gyroscope Trajectory\")\n",
    "    ax2.set_xlabel(\"Gx (rad/s)\")\n",
    "    ax2.set_ylabel(\"Gy (rad/s)\")\n",
    "    ax2.set_zlabel(\"Gz (rad/s)\")\n",
    "    fig.colorbar(scatter2, ax=ax2, label=\"Time Gradient\")\n",
    "\n",
    "    # Set Chart Title\n",
    "    fig.suptitle(f\"{LIFT_NAMES[lN]} - {LIFT_CLASSES[lC]}\", fontsize=16)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    if save:\n",
    "        fig.savefig(f\"{PLOT_DIR}/{LIFT_NAMES[lN]}_{LIFT_CLASSES[lC]}_3d.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def animate_3d_trajectory(data, filename, duration_factor=1):\n",
    "    set_matplotlib_backend(\"notebook\")\n",
    "    d = data[\"tSD\"]\n",
    "    lN = data[\"lN\"]\n",
    "    lC = data[\"lC\"]\n",
    "    time, aX, aY, aZ, gX, gY, gZ = create_lists(d)\n",
    "\n",
    "    # Duration and timing\n",
    "    base_duration = 5  # Base duration in seconds\n",
    "    total_duration = base_duration * duration_factor\n",
    "    interval = (total_duration * 1000) / len(time)  # Interval in milliseconds\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 8))\n",
    "    ax = fig.add_subplot(131, projection=\"3d\")  # Acceleration\n",
    "    ax2 = fig.add_subplot(132, projection=\"3d\")  # Gyroscope\n",
    "    ax3 = fig.add_subplot(133, projection=\"3d\")  # Position\n",
    "\n",
    "    # Set axis limits dynamically to encompass all data\n",
    "    buffer = 0.1  # Add a 10% buffer for better visibility\n",
    "    ax.set_xlim([min(aX) - buffer, max(aX) + buffer])\n",
    "    ax.set_ylim([min(aY) - buffer, max(aY) + buffer])\n",
    "    ax.set_zlim([min(aZ) - buffer, max(aZ) + buffer])\n",
    "\n",
    "    ax2.set_xlim([min(gX) - buffer, max(gX) + buffer])\n",
    "    ax2.set_ylim([min(gY) - buffer, max(gY) + buffer])\n",
    "    ax2.set_zlim([min(gZ) - buffer, max(gZ) + buffer])\n",
    "\n",
    "    # Styling for Acceleration\n",
    "    scatter_acc = ax.scatter([], [], [], c=[], cmap=\"viridis\", s=100)\n",
    "    (trajectory_acc,) = ax.plot([], [], [], color=\"blue\", alpha=0.6, linewidth=2)\n",
    "    ax.grid(True)\n",
    "    ax.set_title(\"3D Acceleration Animation\", fontsize=14)\n",
    "    ax.set_xlabel(\"Ax (m/s^2)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Ay (m/s^2)\", fontsize=12)\n",
    "    ax.set_zlabel(\"Az (m/s^2)\", fontsize=12)\n",
    "\n",
    "    # Styling for Gyroscope\n",
    "    scatter_gyro = ax2.scatter([], [], [], c=[], cmap=\"plasma\", s=100)\n",
    "    (trajectory_gyro,) = ax2.plot([], [], [], color=\"red\", alpha=0.6, linewidth=2)\n",
    "    ax2.grid(True)\n",
    "    ax2.set_title(\"3D Gyroscope Animation\", fontsize=14)\n",
    "    ax2.set_xlabel(\"Gx (rad/s)\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Gy (rad/s)\", fontsize=12)\n",
    "    ax2.set_zlabel(\"Gz (rad/s)\", fontsize=12)\n",
    "\n",
    "    # Set Chart Title\n",
    "    fig.suptitle(f\"{LIFT_NAMES[lN]} - {LIFT_CLASSES[lC]}\", fontsize=16)\n",
    "\n",
    "    def init():\n",
    "        scatter_acc.set_offsets(np.empty((0, 3)))\n",
    "        scatter_gyro.set_offsets(np.empty((0, 3)))\n",
    "        trajectory_acc.set_data([], [])\n",
    "        trajectory_acc.set_3d_properties([])\n",
    "        trajectory_gyro.set_data([], [])\n",
    "        trajectory_gyro.set_3d_properties([])\n",
    "\n",
    "        return (\n",
    "            scatter_acc,\n",
    "            scatter_gyro,\n",
    "            trajectory_acc,\n",
    "            trajectory_gyro,\n",
    "        )\n",
    "\n",
    "    def update(frame):\n",
    "        # Acceleration\n",
    "        x_acc, y_acc, z_acc = aX[: frame + 1], aY[: frame + 1], aZ[: frame + 1]\n",
    "        scatter_acc._offsets3d = (x_acc, y_acc, z_acc)\n",
    "        trajectory_acc.set_data(x_acc, y_acc)\n",
    "        trajectory_acc.set_3d_properties(z_acc)\n",
    "\n",
    "        # Gyroscope\n",
    "        x_gyro, y_gyro, z_gyro = gX[: frame + 1], gY[: frame + 1], gZ[: frame + 1]\n",
    "        scatter_gyro._offsets3d = (x_gyro, y_gyro, z_gyro)\n",
    "        trajectory_gyro.set_data(x_gyro, y_gyro)\n",
    "        trajectory_gyro.set_3d_properties(z_gyro)\n",
    "\n",
    "        return (\n",
    "            scatter_acc,\n",
    "            scatter_gyro,\n",
    "            trajectory_acc,\n",
    "            trajectory_gyro\n",
    "        )\n",
    "\n",
    "    ani = FuncAnimation(\n",
    "        fig, update, frames=len(time), init_func=init, blit=False, interval=interval\n",
    "    )\n",
    "\n",
    "    # Save the animation\n",
    "    ani.save(filename, writer=\"ffmpeg\", fps=1000 // interval)\n",
    "    print(f\"Animation saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the directory already exists\n",
    "if not os.path.exists(ANIM_DIR):\n",
    "    try:\n",
    "        # Create the directory\n",
    "        os.makedirs(ANIM_DIR)\n",
    "        print(f\"Directory '{ANIM_DIR}' created successfully.\")\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that occur during directory creation\n",
    "        print(f\"Error creating directory '{ANIM_DIR}': {e}\")\n",
    "else:\n",
    "    print(f\"Directory '{ANIM_DIR}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16: Lift Instability\n",
    "# 28: Swinging Weight\n",
    "# 55: Off-Axis\n",
    "# 99: Partial Motion\n",
    "# 221: No Lift\n",
    "# 231: Perfect Form\n",
    "SAVE = False\n",
    "if SAVE:\n",
    "    for i in [16, 28, 55, 99, 221, 231]:\n",
    "        data = DATASET[i]\n",
    "        EXT = \".mp4\"\n",
    "        anim_filename = f\"{ANIM_DIR}/{str(LIFT_NAMES[data['lN']]).replace('.json','')}_{str(LIFT_CLASSES[data['lC']])}_{EXT}\".replace(\n",
    "            \" \", \"_\"\n",
    "        )\n",
    "        animate_3d_trajectory(data, filename=anim_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video(\"anim/Dumbbell_Curl_Off-Axis_.mp4\", embed=True)\n",
    "# Video(\"anim/Dumbbell_Curl_Lift_Instability_.mp4\", embed=True)\n",
    "# Video(\"anim/Dumbbell_Curl_Partial_Motion_.mp4\", embed=True)\n",
    "# Video(\"anim/Dumbbell_Curl_Swinging_Weight_.mp4\", embed=True)\n",
    "# # Video(\"anim/Dumbbell_Curl_Perfect_Form_.mp4\", embed=True)\n",
    "# Video(\"anim/Dumbbell_Curl_No_Lift_.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "SHOW = False\n",
    "if SAVE or SHOW:\n",
    "    for i in [16, 28, 55, 99, 231]:\n",
    "        data = DATASET[i]\n",
    "        plot_3d_trajectory(data, show=SHOW, save=SAVE)\n",
    "        plot_flat_dataset(data, show=SHOW, save=SAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split & Pre-Process Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PERCENTAGE = 10\n",
    "VALIDATION_PERCENTAGE = 20\n",
    "TRAIN_PERCENTAGE = 100 - (TEST_PERCENTAGE + VALIDATION_PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(LIFT_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_classes_in_dataset(dataset):\n",
    "    return set([d[\"lC\"] for d in dataset])\n",
    "\n",
    "\n",
    "def get_min_classes_covered(*args):\n",
    "    return min(len(check_classes_in_dataset(arg)) for arg in args)\n",
    "\n",
    "\n",
    "test_dataset = []\n",
    "validation_dataset = []\n",
    "train_dataset = []\n",
    "\n",
    "# Ensure that the test, validation, and train datasets contain all classes\n",
    "while (\n",
    "    get_min_classes_covered(test_dataset, validation_dataset, train_dataset)\n",
    "    != NUM_CLASSES\n",
    "):\n",
    "    shuffled_dataset = DATASET.copy()\n",
    "    random.shuffle(shuffled_dataset)\n",
    "\n",
    "    test_count = math.floor(len(shuffled_dataset) * TEST_PERCENTAGE / 100)\n",
    "    validation_count = math.floor(len(shuffled_dataset) * VALIDATION_PERCENTAGE / 100)\n",
    "    train_count = len(shuffled_dataset) - test_count - validation_count\n",
    "\n",
    "    test_dataset = shuffled_dataset[:test_count]\n",
    "    validation_dataset = shuffled_dataset[test_count : test_count + validation_count]\n",
    "    train_dataset = shuffled_dataset[test_count + validation_count :]\n",
    "\n",
    "print(\n",
    "    f\"Test Dataset: {len(test_dataset)} contains {len(check_classes_in_dataset(test_dataset))} classes.\"\n",
    ")\n",
    "print(\n",
    "    f\"Validation Dataset: {len(validation_dataset)} contains {len(check_classes_in_dataset(validation_dataset))} classes.\"\n",
    ")\n",
    "print(\n",
    "    f\"Train Dataset: {len(train_dataset)} contains {len(check_classes_in_dataset(train_dataset))} classes.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Size of Model\n",
    "VECTOR_X = len(DATASET[0][\"tSD\"][0]) - 1  # Remove the time column\n",
    "VECTOR_Y = 200  # Number of samples in the y-axis (downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_array(dataset):\n",
    "    # drops the time column\n",
    "    tSD = dataset[\"tSD\"]\n",
    "    label = dataset[\"lC\"]\n",
    "    return label, np.array(\n",
    "        [[ts[\"aX\"], ts[\"aY\"], ts[\"aZ\"], ts[\"gX\"], ts[\"gY\"], ts[\"gZ\"]] for ts in tSD]\n",
    "    )\n",
    "\n",
    "\n",
    "# Fill the head and tail of the dataset with the first and last sample.\n",
    "def extend_dataset_length(dataset, target_length=1000):\n",
    "    if 0 < (n := target_length - len(dataset)):\n",
    "        head_padding = np.tile(dataset[0:1], (n // 2, 1))\n",
    "        tail_padding = np.tile(dataset[-1:], (n - n // 2, 1))\n",
    "        return np.concatenate([head_padding, dataset, tail_padding])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def downsample_dataset(dataset, target_length=VECTOR_Y, algo=\"avg\"):\n",
    "    # Algo Options: \"avg\", \"rand\"\n",
    "    if algo not in [\"avg\", \"rand\"]:\n",
    "        raise ValueError(f\"Invalid algorithm: {algo}. Use 'avg' or 'rand'.\")\n",
    "\n",
    "    n_time_steps, n_features = dataset.shape\n",
    "\n",
    "    # Handle case where target length is greater than dataset length\n",
    "    if target_length >= n_time_steps:\n",
    "        return extend_dataset_length(dataset, target_length)\n",
    "\n",
    "    window_size = n_time_steps // target_length\n",
    "\n",
    "    # Downsample using the chosen algorithm\n",
    "    if algo == \"avg\":\n",
    "        return np.array(\n",
    "            [\n",
    "                np.mean(\n",
    "                    dataset[i : i + window_size, :], axis=0\n",
    "                )  # Mean across each window\n",
    "                for i in range(0, n_time_steps - window_size + 1, window_size)\n",
    "            ]\n",
    "        )[\n",
    "            :target_length\n",
    "        ]  # Ensure we get exactly target_length samples\n",
    "\n",
    "    elif algo == \"rand\":\n",
    "        # For each window, select one random sample\n",
    "        downsampled = np.array(\n",
    "            [\n",
    "                dataset[\n",
    "                    i + np.random.randint(0, window_size), :\n",
    "                ]  # Random sample from window\n",
    "                for i in range(0, n_time_steps - window_size + 1, window_size)\n",
    "            ]\n",
    "        )[\n",
    "            :target_length\n",
    "        ]  # Ensure we get exactly target_length samples\n",
    "        return downsampled\n",
    "\n",
    "\n",
    "def normalize_sensor_data(data, acc_min, acc_max, gyro_min, gyro_max):\n",
    "    # Add samples dimension if input is single sample\n",
    "    if len(data.shape) == 2:\n",
    "        data = np.expand_dims(data, axis=0)\n",
    "\n",
    "    # Split accelerometer and gyroscope data\n",
    "    acc_data = data[..., :3]  # aX, aY, aZ\n",
    "    gyro_data = data[..., 3:]  # gX, gY, gZ\n",
    "\n",
    "\n",
    "    # Avoid division by zero by adding small epsilon\n",
    "    eps = 1e-7\n",
    "    acc_range = np.maximum(acc_max - acc_min, eps)\n",
    "    gyro_range = np.maximum(gyro_max - gyro_min, eps)\n",
    "\n",
    "    # Normalize to [0,1] range\n",
    "    acc_normalized = (acc_data - acc_min) / acc_range\n",
    "    gyro_normalized = (gyro_data - gyro_min) / gyro_range\n",
    "\n",
    "    normalized_data = np.concatenate([acc_normalized, gyro_normalized], axis=-1)\n",
    "\n",
    "    # Remove samples dimension if input was single sample\n",
    "    if normalized_data.shape[0] == 1:\n",
    "        normalized_data = normalized_data[0]\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "def signal_augmentation(data):\n",
    "    # Convert data to tensor and ensure float32 dtype\n",
    "    data = tf.cast(tf.convert_to_tensor(data), tf.float32)\n",
    "\n",
    "    # Define simpler augmentations with consistent dtypes\n",
    "    augs = [\n",
    "        (\n",
    "            \"noise\",\n",
    "            lambda data: data\n",
    "            + tf.cast(\n",
    "                tf.random.normal(data.shape, mean=0, stddev=0.03), dtype=tf.float32\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"mag_scale\",\n",
    "            lambda data: data\n",
    "            * tf.cast(tf.random.uniform([], minval=0.9, maxval=1.1), dtype=tf.float32),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Randomly select 1-2 augmentations\n",
    "    num_augs = tf.random.uniform([], minval=1, maxval=3, dtype=tf.int32)\n",
    "    indices = tf.random.shuffle(tf.range(len(augs)))[:num_augs]\n",
    "\n",
    "    augmented_data = data\n",
    "    for idx in indices:\n",
    "        _, aug_fn = augs[idx]\n",
    "        augmented_data = aug_fn(augmented_data)\n",
    "\n",
    "    return augmented_data\n",
    "\n",
    "\n",
    "def rand_augment_dataset(data, min_copies=1, max_copies=3):\n",
    "    # Keep original\n",
    "    augmented_dataset = [data]\n",
    "\n",
    "    # Generate fixed number of copies\n",
    "    num_copies = np.random.randint(min_copies, max_copies + 1)\n",
    "\n",
    "    for _ in range(num_copies):\n",
    "        augmented_dataset.append(signal_augmentation(data))\n",
    "\n",
    "    return augmented_dataset\n",
    "\n",
    "\n",
    "def augment_dataset(data, num_copies):\n",
    "    \"\"\"\n",
    "    Generate exactly num_copies augmented versions of the data\n",
    "    \"\"\"\n",
    "    augmented_dataset = []\n",
    "    for _ in range(num_copies):\n",
    "        augmented_dataset.append(signal_augmentation(data))\n",
    "    return augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_acc_gyro_min_max(dataset, scale_factor = 1):\n",
    "    acc_min, acc_max, gyro_min, gyro_max = float('inf'), float('-inf'), float('inf'), float('-inf')\n",
    "    for data in dataset:\n",
    "        _, data_array = convert_to_array(data)\n",
    "        acc_data = data_array[..., :3]\n",
    "        gyro_data = data_array[..., 3:]\n",
    "        acc_min = min(acc_min, np.min(acc_data) * scale_factor) \n",
    "        acc_max = max(acc_max, np.max(acc_data) * scale_factor)\n",
    "        gyro_min = min(gyro_min, np.min(gyro_data) * scale_factor)\n",
    "        gyro_max = max(gyro_max, np.max(gyro_data) * scale_factor)\n",
    "    return acc_min, acc_max, gyro_min, gyro_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_MIN, ACC_MAX, GYRO_MIN, GYRO_MAX = gather_acc_gyro_min_max(DATASET, scale_factor = 1.25)\n",
    "print(\"Normalization Parameters:\")\n",
    "print(f\"Acc Min: {ACC_MIN}, Acc Max: {ACC_MAX}, Gyro Min: {GYRO_MIN}, Gyro Max: {GYRO_MAX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, augment=False, augmentation_factor=2, acc_min=ACC_MIN, acc_max=ACC_MAX, gyro_min=GYRO_MIN, gyro_max=GYRO_MAX):\n",
    "    \"\"\"\n",
    "    Preprocess dataset with balanced class augmentation.\n",
    "    When augment=False, uses minimum class count for balance.\n",
    "    When augment=True, augments all classes to max_count * augmentation_factor.\n",
    "    \"\"\"\n",
    "    # First pass: Count class distribution\n",
    "    class_counts = {}\n",
    "    for data in dataset:\n",
    "        label = data[\"lC\"]\n",
    "        class_counts[label] = class_counts.get(label, 0) + 1\n",
    "\n",
    "    print(\"Initial class distribution:\", class_counts)\n",
    "\n",
    "    # Set target count based on augmentation flag\n",
    "    if augment:\n",
    "        base_count = max(class_counts.values())\n",
    "        target_count = base_count * augmentation_factor\n",
    "    else:\n",
    "        # When not augmenting, use minimum class count\n",
    "        target_count = min(class_counts.values())\n",
    "\n",
    "    X = []  # Sensor Data\n",
    "    y = []  # Labels\n",
    "\n",
    "    # Process each class separately\n",
    "    for label in class_counts.keys():\n",
    "        # Get all samples for this class\n",
    "        class_samples = [data for data in dataset if data[\"lC\"] == label]\n",
    "\n",
    "        # Shuffle the samples to ensure random selection when limiting\n",
    "        random.shuffle(class_samples)\n",
    "\n",
    "        # Process original samples first (up to target_count)\n",
    "        samples_processed = 0\n",
    "        for data in class_samples:\n",
    "            if samples_processed >= target_count and not augment:\n",
    "                break\n",
    "\n",
    "            label, data_array = convert_to_array(data)\n",
    "            data_array = extend_dataset_length(data_array, 1000)\n",
    "            data_array = downsample_dataset(data_array, VECTOR_Y, \"avg\")\n",
    "            data_array = normalize_sensor_data(data_array, acc_min, acc_max, gyro_min, gyro_max)\n",
    "\n",
    "            if data_array is not None and label is not None:\n",
    "                X.append(data_array)\n",
    "                y.append(label)\n",
    "                samples_processed += 1\n",
    "\n",
    "        if augment:\n",
    "            # Calculate remaining samples needed for augmentation\n",
    "            current_class_count = sum(1 for label_y in y if label_y == label)\n",
    "            samples_needed = target_count - current_class_count\n",
    "\n",
    "            if samples_needed > 0:\n",
    "                sample_idx = 0\n",
    "                for _ in range(samples_needed):\n",
    "                    original_data = class_samples[sample_idx % len(class_samples)]\n",
    "                    sample_idx += 1\n",
    "\n",
    "                    label, data_array = convert_to_array(original_data)\n",
    "                    data_array = extend_dataset_length(data_array, 1000)\n",
    "                    data_array = downsample_dataset(data_array, VECTOR_Y, \"avg\")\n",
    "                    augmented = signal_augmentation(data_array)\n",
    "                    augmented = normalize_sensor_data(augmented, acc_min, acc_max, gyro_min, gyro_max)\n",
    "\n",
    "                    if augmented is not None and label is not None:\n",
    "                        X.append(augmented)\n",
    "                        y.append(label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Validate arrays\n",
    "    assert len(X) == len(y), f\"Mismatch in lengths: X={len(X)}, y={len(y)}\"\n",
    "\n",
    "    # Verify final distribution\n",
    "    final_counts = {}\n",
    "    for label in y:\n",
    "        final_counts[label] = final_counts.get(label, 0) + 1\n",
    "    print(\"Final class distribution:\", final_counts)\n",
    "\n",
    "    # Additional validation\n",
    "    print(f\"Shape Checks:\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(f\"Number of unique labels: {len(np.unique(y))}\")\n",
    "\n",
    "    # Verify shapes\n",
    "    for i in range(len(X)):\n",
    "        if X[i].shape != (200, 6):\n",
    "            raise ValueError(f\"Invalid shape at index {i}: {X[i].shape}\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normalization_comparison(original_sample, normalized_sample):\n",
    "    \"\"\"\n",
    "    Plot original vs normalized data for a single sample with left/right splits\n",
    "    \"\"\"\n",
    "    # Split feature names into acceleration and gyroscope\n",
    "    accel_features = [\"aX\", \"aY\", \"aZ\"]\n",
    "    gyro_features = [\"gX\", \"gY\", \"gZ\"]\n",
    "\n",
    "    # Create figure with 2x2 subplot grid\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "    # Plot original acceleration data (top left)\n",
    "    for i, feature in enumerate(accel_features):\n",
    "        axes[0, 0].plot(original_sample[:, i], label=feature)\n",
    "    axes[0, 0].set_title(\"Original Acceleration Data\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Plot original gyroscope data (top right)\n",
    "    for i, feature in enumerate(gyro_features):\n",
    "        axes[0, 1].plot(original_sample[:, i + 3], label=feature)\n",
    "    axes[0, 1].set_title(\"Original Gyroscope Data\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Plot normalized acceleration data (bottom left)\n",
    "    for i, feature in enumerate(accel_features):\n",
    "        axes[1, 0].plot(normalized_sample[:, i], label=feature)\n",
    "    axes[1, 0].set_title(\"Normalized Acceleration Data\")\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Plot normalized gyroscope data (bottom right)\n",
    "    for i, feature in enumerate(gyro_features):\n",
    "        axes[1, 1].plot(normalized_sample[:, i + 3], label=feature)\n",
    "    axes[1, 1].set_title(\"Normalized Gyroscope Data\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_NORMALIZATION = True\n",
    "if SHOW_NORMALIZATION:\n",
    "    # Visualize normalization for a few samples\n",
    "    for i in [16, 28, 55, 99, 221, 231]:  # Show rep sample\n",
    "        print(\n",
    "            f\"\\nSample {i} - {LIFT_NAMES[DATASET[i]['lN']]} - {LIFT_CLASSES[DATASET[i]['lC']]}\"\n",
    "        )\n",
    "        label, original_data = convert_to_array(DATASET[i])\n",
    "        extended_data = extend_dataset_length(original_data, 1000)\n",
    "        downsampled_data = downsample_dataset(extended_data, VECTOR_Y, \"avg\")\n",
    "        normalized_data = normalize_sensor_data(downsampled_data, ACC_MIN, ACC_MAX, GYRO_MIN, GYRO_MAX)\n",
    "        plot_normalization_comparison(downsampled_data, normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"VALIDATION DATASET\")\n",
    "VALIDATION_X, VALIDATION_Y = preprocess_dataset(validation_dataset, augment=False)\n",
    "print(\"--------------------------------\")\n",
    "print(\"TEST DATASET\")\n",
    "TEST_X, TEST_Y = preprocess_dataset(test_dataset, augment=True)\n",
    "print(\"--------------------------------\")\n",
    "print(\"TRAIN DATASET\")\n",
    "TRAIN_X, TRAIN_Y = preprocess_dataset(train_dataset, augment=True)\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(f\"Validation: {VALIDATION_X.shape} Labels: {VALIDATION_Y.shape})\")\n",
    "print(f\"Test: {TEST_X.shape} Labels: {TEST_Y.shape})\")\n",
    "print(f\"Train: {TRAIN_X.shape} Labels: {TRAIN_Y.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distribution(data, labels):\n",
    "    first_sample = data[0] \n",
    "    first_label = labels[0] \n",
    "\n",
    "    # Create a figure with two subplots to visualize the distribution of features\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "    # Plot values for acceleration features (0:3)\n",
    "    axes[0].plot(first_sample[:, 0], marker=\"o\", label=\"aX\")\n",
    "    axes[0].plot(first_sample[:, 1], marker=\"o\", label=\"aY\")\n",
    "    axes[0].plot(first_sample[:, 2], marker=\"o\", label=\"aZ\")\n",
    "    axes[0].set_title(\"Acceleration Features\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Plot values for gyroscope features (3:6)\n",
    "    axes[1].plot(first_sample[:, 3], marker=\"o\", label=\"gX\")\n",
    "    axes[1].plot(first_sample[:, 4], marker=\"o\", label=\"gY\")\n",
    "    axes[1].plot(first_sample[:, 5], marker=\"o\", label=\"gZ\")\n",
    "    axes[1].set_title(\"Gyroscope Features\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    # Adjust layout and display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function gated by an if statement\n",
    "SHOW_FEATURE_PLOT = False\n",
    "if SHOW_FEATURE_PLOT:\n",
    "    plot_feature_distribution(VALIDATION_X, VALIDATION_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Label Output\n",
    "labels = sorted(set(TEST_Y).union(set(VALIDATION_Y)).union(set(TRAIN_Y)))\n",
    "labelToInt = {}\n",
    "currInt = 0\n",
    "for label in labels:\n",
    "    labelToInt[label] = currInt\n",
    "    currInt = currInt + 1\n",
    "intToLabel = {v: k for k, v in labelToInt.items()}\n",
    "print(intToLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_input_range(dataset):\n",
    "  min_val = float('inf')\n",
    "  max_val= -float('inf')\n",
    "  for data in dataset:\n",
    "    min_val = min(min_val, np.min(data))\n",
    "    max_val = max(max_val, np.max(data))\n",
    "  return min_val, max_val\n",
    "\n",
    "min_val, max_val = gather_input_range(TRAIN_X)\n",
    "print(f\"Min: {min_val}, Max: {max_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intToLabel.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding of Labels\n",
    "TEST_Y_cat = tf.keras.utils.to_categorical([labelToInt[y] for y in TEST_Y])\n",
    "VALIDATION_Y_cat = tf.keras.utils.to_categorical([labelToInt[y] for y in VALIDATION_Y])\n",
    "TRAIN_Y_cat = tf.keras.utils.to_categorical([labelToInt[y] for y in TRAIN_Y])\n",
    "\n",
    "# Define Batch Size\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE_BUFFER_SIZE = 256\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "# Create tensorflow datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((TRAIN_X, TRAIN_Y_cat))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((VALIDATION_X, VALIDATION_Y_cat))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((TEST_X, TEST_Y_cat))\n",
    "\n",
    "train_ds = (\n",
    "    train_ds.cache().shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(AUTO)\n",
    ")\n",
    "val_ds = val_ds.cache().batch(BATCH_SIZE).prefetch(AUTO)\n",
    "test_ds = test_ds.cache().batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "\n",
    "for x, y in train_ds.take(1):\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "# The OUT_OF_RANGE error is OKAY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (VECTOR_Y, VECTOR_X)\n",
    "\n",
    "\n",
    "def create_model(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(64, kernel_size=8, padding=\"same\")(inputs)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=4)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(32, kernel_size=6, padding=\"same\")(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=4)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(16, kernel_size=4, padding=\"same\")(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(32)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(16)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_ds)\n",
    "\n",
    "# Define learning rate parameters\n",
    "initial_learning_rate = 0.001\n",
    "warmup_epochs = 50\n",
    "warmup_steps = warmup_epochs * steps_per_epoch\n",
    "first_decay_epochs = 500\n",
    "first_decay_steps = first_decay_epochs * steps_per_epoch\n",
    "\n",
    "# Cosine Decay with Warmup schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "    initial_learning_rate,\n",
    "    first_decay_steps=first_decay_steps,\n",
    "    t_mul=2,  # Each restart cycle will be 1.5x longer than the previous\n",
    "    m_mul=0.98,  # Each restart will have 0.95x the max learning rate of the previous\n",
    "    alpha=0.2,  # Minimum learning rate will be 10% of initial\n",
    ")\n",
    "\n",
    "# Update optimizer with new schedule\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=lr_schedule,\n",
    "    weight_decay=0.0005,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    ")\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "EPOCHS = 5000  # MAKE SURE THAT THE VAL_THRESHOLD IS MET\n",
    "\n",
    "checkpoint_filepath = os.path.join(CHKPT_DIR, \"cp-{epoch:04d}.keras\")\n",
    "os.makedirs(CHKPT_DIR, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "val_threshold = 0.97\n",
    "\n",
    "\n",
    "class EarlyStopping(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, threshold=0.98, patience=10):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.patience = patience  # Number of epochs to maintain threshold\n",
    "        self.threshold_epochs = 0  # Counter for epochs above threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get(\"val_accuracy\") > self.threshold:\n",
    "            self.threshold_epochs += 1\n",
    "            if self.threshold_epochs >= self.patience:\n",
    "                print(\n",
    "                    f\"\\nMaintained {self.threshold} accuracy for {self.patience} epochs, stopping training!\"\n",
    "                )\n",
    "                self.model.stop_training = True\n",
    "        else:\n",
    "            self.threshold_epochs = 0\n",
    "\n",
    "\n",
    "early_stopping_callback = EarlyStopping(threshold=val_threshold, patience=10) \n",
    "\n",
    "log_dir = os.path.join(LOG_DIR, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir={log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[checkpoint_callback, tensorboard_callback, early_stopping_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "model.export(f\"{SAVED_MODEL_FILENAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_QUANTIZATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a non quantized model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(f\"{SAVED_MODEL_FILENAME}\")\n",
    "model_no_quant_tflite = converter.convert()\n",
    "\n",
    "with open(FLOAT_TFL_MODEL_FILENAME, \"wb\") as f:\n",
    "    f.write(model_no_quant_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_QUANTIZATION:\n",
    "    def representative_dataset():\n",
    "        unique_samples = np.unique(TRAIN_X, axis=0)\n",
    "        for sample in unique_samples:\n",
    "                yield [sample.astype(np.float32)]\n",
    "\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(f\"{SAVED_MODEL_FILENAME}\")\n",
    "        \n",
    "    # Basic optimization settings\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    # Force full INT8 quantization\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    ]\n",
    "\n",
    "    # Representative dataset is required for full integer quantization\n",
    "    converter.representative_dataset = representative_dataset\n",
    "\n",
    "    # Force input/output to be INT8\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "\n",
    "    # Enable full integer quantization\n",
    "    converter.full_integer_quantization = True\n",
    "\n",
    "    # Metal-specific settings\n",
    "    converter._experimental_disable_per_channel = True\n",
    "    converter.target_spec.supported_types = [tf.int8]\n",
    "\n",
    "    # Convert model\n",
    "    model_tflite = converter.convert()\n",
    "\n",
    "    with open(QUANTIZED_TFL_MODEL_FILENAME, \"wb\") as f:\n",
    "        f.write(model_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model sizes\n",
    "def get_dir_size(dir_path):\n",
    "    total_size = 0\n",
    "    for dirpath, _, filenames in os.walk(dir_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "\n",
    "# Calculate sizes\n",
    "size_tf = get_dir_size(SAVED_MODEL_FILENAME)\n",
    "size_no_quant_tflite = os.path.getsize(FLOAT_TFL_MODEL_FILENAME)\n",
    "if not SKIP_QUANTIZATION:\n",
    "    size_tflite = os.path.getsize(QUANTIZED_TFL_MODEL_FILENAME)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "size_comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"TensorFlow\", \"TensorFlow Lite\", \"TensorFlow Lite Quantized\"],\n",
    "        \"Size\": [\n",
    "            f\"{size_tf:,} bytes\",\n",
    "            f\"{size_no_quant_tflite:,} bytes\",\n",
    "            f\"{size_tflite:,} bytes\" if not SKIP_QUANTIZATION else \"N/A\",\n",
    "        ],\n",
    "        \"Reduction\": [\n",
    "            \"\",\n",
    "            f\"(reduced by {size_tf - size_no_quant_tflite:,} bytes)\",\n",
    "            f\"(reduced by {size_no_quant_tflite - size_tflite:,} bytes)\" if not SKIP_QUANTIZATION else \"\",\n",
    "        ],\n",
    "    }\n",
    ").set_index(\"Model\")\n",
    "\n",
    "# Display comparison\n",
    "size_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tflite_model(tflite_model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    print(\"== Input Details ==\")\n",
    "    for detail in input_details:\n",
    "        print(f\"Name: {detail['name']}\")\n",
    "        print(f\"Shape: {detail['shape']}\")\n",
    "        print(f\"Type: {detail['dtype']}\")\n",
    "        print(f\"Quantization: {detail['quantization']}\")\n",
    "        print()\n",
    "\n",
    "    print(\"== Output Details ==\")\n",
    "    for detail in output_details:\n",
    "        print(f\"Name: {detail['name']}\")\n",
    "        print(f\"Shape: {detail['shape']}\")\n",
    "        print(f\"Type: {detail['dtype']}\")\n",
    "        print(f\"Quantization: {detail['quantization']}\")\n",
    "        print()\n",
    "\n",
    "    # Print model size\n",
    "    print(f\"Model Size: {os.path.getsize(tflite_model_path) / 1024:.2f} KB\")\n",
    "\n",
    "\n",
    "# Check both models\n",
    "print(\"=== Float Model ===\")\n",
    "check_tflite_model(FLOAT_TFL_MODEL_FILENAME)\n",
    "if not SKIP_QUANTIZATION:\n",
    "    print(\"\\n=== Quantized Model ===\")\n",
    "    check_tflite_model(QUANTIZED_TFL_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models():\n",
    "\n",
    "    def run_tflite_inference(interpreter, input_data, is_quantized=False, debug=False):\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "\n",
    "        # Debug: Print input details\n",
    "        if debug:\n",
    "            print(f\"Input details: {input_details}\")\n",
    "            print(f\"Output details: {output_details}\")\n",
    "\n",
    "        input_data = input_data.numpy()\n",
    "\n",
    "        # Handle input scaling for quantized model\n",
    "        if is_quantized:\n",
    "            input_scale = input_details[0][\"quantization\"][0]\n",
    "            input_zero_point = input_details[0][\"quantization\"][1]\n",
    "            input_data = input_data / input_scale + input_zero_point\n",
    "            input_data = input_data.astype(np.int8)\n",
    "        else:\n",
    "            input_data = input_data.astype(np.float32)\n",
    "\n",
    "        predictions = []\n",
    "        input_data_reshaped = input_data.reshape(\n",
    "            (-1,) + tuple(input_details[0][\"shape\"][1:])\n",
    "        )\n",
    "\n",
    "        # Process batches\n",
    "        for i in range(input_data_reshaped.shape[0]):\n",
    "            sample = input_data_reshaped[i : i + 1]\n",
    "            interpreter.set_tensor(input_details[0][\"index\"], sample)\n",
    "            interpreter.invoke()\n",
    "            output = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "            # Dequantize output if needed\n",
    "            if is_quantized:\n",
    "                output_scale = output_details[0][\"quantization\"][0]\n",
    "                output_zero_point = output_details[0][\"quantization\"][1]\n",
    "                output = (output.astype(np.float32) - output_zero_point) * output_scale\n",
    "\n",
    "            predictions.append(output[0])\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def calculate_metrics(y_true, y_pred, class_names):\n",
    "        \"\"\"\n",
    "        Calculates classification metrics with proper handling of class names.\n",
    "        Args:\n",
    "            y_true: True labels\n",
    "            y_pred: Predicted labels\n",
    "            class_names: Dictionary mapping class indices to names\n",
    "        Returns:\n",
    "            Tuple of (accuracy, precision, recall, f1)\n",
    "        \"\"\"\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=\"weighted\", labels=list(class_names.keys())\n",
    "        )\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    results = {}\n",
    "    y_true = np.argmax(TEST_Y_cat, axis=1)\n",
    "\n",
    "    # Test each model variant\n",
    "    models_to_test = [\n",
    "        (\"TensorFlow\", None, False, True),\n",
    "        (\"TFLite Float\", FLOAT_TFL_MODEL_FILENAME, False, True),\n",
    "        # (\"TFLite Quantized\", QUANTIZED_TFL_MODEL_FILENAME, True, True),\n",
    "    ]\n",
    "\n",
    "    for model_name, model_path, is_quantized, debug in models_to_test:\n",
    "        print(f\"\\nTesting {model_name}...\")\n",
    "\n",
    "        if model_name == \"TensorFlow\":\n",
    "            # Handle TensorFlow model\n",
    "            predictions = []\n",
    "            for x, _ in test_ds:\n",
    "                pred = model.predict(x, verbose=0)\n",
    "                predictions.extend(pred)\n",
    "        else:\n",
    "            # Handle TFLite models\n",
    "            interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "            interpreter.allocate_tensors()\n",
    "            input_details = interpreter.get_input_details()\n",
    "            print(\"Quantization parameters:\", input_details[0][\"quantization\"])\n",
    "\n",
    "            predictions = []\n",
    "            for x, _ in test_ds:\n",
    "                pred_batch = run_tflite_inference(interpreter, x, is_quantized, debug)\n",
    "                predictions.extend(pred_batch)\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        pred_classes = np.argmax(predictions, axis=1)\n",
    "        metrics = calculate_metrics(y_true, pred_classes, intToLabel)\n",
    "\n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            \"accuracy\": metrics[0],\n",
    "            \"precision\": metrics[1],\n",
    "            \"recall\": metrics[2],\n",
    "            \"f1\": metrics[3],\n",
    "        }\n",
    "\n",
    "        # Print class distribution\n",
    "        print(f\"\\n{model_name} Prediction Distribution:\")\n",
    "        unique, counts = np.unique(pred_classes, return_counts=True)\n",
    "        for class_idx, count in zip(unique, counts):\n",
    "            class_name = intToLabel.get(class_idx, f\"Unknown ({class_idx})\")\n",
    "            print(f\"{class_name}: {count} predictions\")\n",
    "\n",
    "    # Create comparison DataFrame\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"Model\": list(results.keys()),\n",
    "            \"Accuracy\": [f\"{results[model]['accuracy']*100:.2f}%\" for model in results],\n",
    "            \"Precision\": [\n",
    "                f\"{results[model]['precision']*100:.2f}%\" for model in results\n",
    "            ],\n",
    "            \"Recall\": [f\"{results[model]['recall']*100:.2f}%\" for model in results],\n",
    "            \"F1 Score\": [f\"{results[model]['f1']*100:.2f}%\" for model in results],\n",
    "        }\n",
    "    ).set_index(\"Model\")\n",
    "\n",
    "\n",
    "model_comparison = evaluate_models()\n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representative_sample(dataset_x, dataset_y, target_class_idx):\n",
    "    \"\"\"Get a representative sample for a given class\"\"\"\n",
    "    # Find indices where the class matches\n",
    "    class_indices = np.where(np.argmax(dataset_y, axis=1) == target_class_idx)[0]\n",
    "    if len(class_indices) == 0:\n",
    "        return None\n",
    "    # Get the first instance of this class\n",
    "    sample = dataset_x[class_indices[0]]\n",
    "    return sample\n",
    "\n",
    "\n",
    "def format_as_c_array(data, array_name):\n",
    "    \"\"\"Format numpy array as C array declaration\"\"\"\n",
    "    rows, cols = data.shape\n",
    "    c_array = f\"float {array_name}[{rows}][{cols}] = {{\\n\"\n",
    "\n",
    "    for i in range(rows):\n",
    "        c_array += \"    {\"\n",
    "        c_array += \", \".join(f\"{x:.6f}\" for x in data[i])\n",
    "        c_array += \"},\\n\"\n",
    "\n",
    "    c_array += \"};\\n\"\n",
    "    return c_array\n",
    "\n",
    "\n",
    "# Get samples for each class\n",
    "class_samples = {\n",
    "    \"lift_instability\": get_representative_sample(\n",
    "        TRAIN_X, TRAIN_Y_cat, labelToInt[\"l_i\"]\n",
    "    ),\n",
    "    \"no_lift\": get_representative_sample(TRAIN_X, TRAIN_Y_cat, labelToInt[\"n_l\"]),\n",
    "    \"off_axis\": get_representative_sample(TRAIN_X, TRAIN_Y_cat, labelToInt[\"o_a\"]),\n",
    "    \"partial_motion\": get_representative_sample(\n",
    "        TRAIN_X, TRAIN_Y_cat, labelToInt[\"p_m\"]\n",
    "    ),\n",
    "    \"perfect_form\": get_representative_sample(TRAIN_X, TRAIN_Y_cat, labelToInt[\"p_f\"]),\n",
    "    \"swinging_weight\": get_representative_sample(\n",
    "        TRAIN_X, TRAIN_Y_cat, labelToInt[\"s_w\"]\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Format as C arrays\n",
    "c_arrays = []\n",
    "for class_name, sample in class_samples.items():\n",
    "    if sample is not None:\n",
    "        array_name = f\"data_2d_{class_name}\"\n",
    "        c_arrays.append(format_as_c_array(sample, array_name))\n",
    "\n",
    "# Print the C arrays\n",
    "print(\"// Generated sample data for each class\")\n",
    "print(\"// Shape: [200][6] - [timesteps][features]\")\n",
    "print(\"// Features order: aX, aY, aZ, gX, gY, gZ\\n\")\n",
    "for array in c_arrays:\n",
    "    print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_CONVERT = QUANTIZED_TFL_MODEL_FILENAME if not SKIP_QUANTIZATION else FLOAT_TFL_MODEL_FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
    "!xxd -i {MODEL_TO_CONVERT} > {TFL_CC_MODEL_FILENAME}\n",
    "\n",
    "REPLACE_TEXT = MODEL_TO_CONVERT.replace('/', '_').replace('.', '_')\n",
    "!sed -i '' 's/'{REPLACE_TEXT}'/g_rep_mate_model_data/g' {TFL_CC_MODEL_FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {TFL_CC_MODEL_FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail {TFL_CC_MODEL_FILENAME}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
