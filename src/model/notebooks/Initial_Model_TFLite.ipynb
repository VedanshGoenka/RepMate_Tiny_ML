{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model TFLite\n",
    "\n",
    "This is the test notebook for the inital model that we will be using for the project.\n",
    "There will be a lot of changes to this model as we go on, and we will be using this notebook to test those changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mac Environment Setup:\n",
    "\n",
    "1. Create a new virtual environment with Python 3.10. Do not use the setup_env.sh script.\n",
    "\n",
    "- `python3.10 -m venv .venv`\n",
    "- `source .venv/bin/activate`\n",
    "- `pip install -r requirements.txt`\n",
    "\n",
    "2. Verify that TensorFlow is working with the GPU.\n",
    "\n",
    "- Check the output of the following cell. You should see something like:\n",
    "  - TensorFlow version: 2.16.2\n",
    "  - Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "  - Result: tf.Tensor([5. 7. 9.], shape=(3,), dtype=float32)\n",
    "  - Metal device set to: Apple M1 Pro\n",
    "  - systemMemory: 16.00 GB\n",
    "  - maxCacheSize: 5.33 GB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Model TFLite\n",
    "\n",
    "## Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Video\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import pydot\n",
    "import graphviz\n",
    "import datetime\n",
    "\n",
    "# Verify TensorFlow installation\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Enable verbose logging\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n",
    "\n",
    "# Check GPU devices\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"Available GPUs:\", gpus)\n",
    "\n",
    "# Perform a computation\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    a = tf.constant([1.0, 2.0, 3.0])\n",
    "    b = tf.constant([4.0, 5.0, 6.0])\n",
    "    c = a + b\n",
    "    print(\"Result:\", c)\n",
    "\n",
    "\n",
    "# The first time this cell is run, make sure that you see:\n",
    "# Metal device set to: Apple M[GPU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories and filenames\n",
    "NOTEBOOK_TAG = \"initial_model\"\n",
    "MODEL_NAME = \"rep_mate.keras\"\n",
    "\n",
    "MODELS_DIR = NOTEBOOK_TAG + \"/models\"\n",
    "DATASET_DIR = NOTEBOOK_TAG + \"/dataset\"\n",
    "CHKPT_DIR = NOTEBOOK_TAG + \"/checkpoints\"\n",
    "LOG_DIR = NOTEBOOK_TAG + \"/logs\"\n",
    "PLOT_DIR = \"plots\"\n",
    "ANIM_DIR = \"anim\"\n",
    "\n",
    "# Create base directories with parents\n",
    "for directory in [MODELS_DIR, DATASET_DIR, CHKPT_DIR, PLOT_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Define model filenames\n",
    "SAVED_MODEL_FILENAME = os.path.join(MODELS_DIR, MODEL_NAME)\n",
    "FLOAT_TFL_MODEL_FILENAME = os.path.join(MODELS_DIR, MODEL_NAME + \"_float.tfl\")\n",
    "QUANTIZED_TFL_MODEL_FILENAME = os.path.join(MODELS_DIR, MODEL_NAME + \".tfl\")\n",
    "TFL_CC_MODEL_FILENAME = os.path.join(MODELS_DIR, MODEL_NAME + \".cc\")\n",
    "\n",
    "# Define dataset directories\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_DIR, \"validation\")\n",
    "TEST_DIR = os.path.join(DATASET_DIR, \"test\")\n",
    "\n",
    "# Create dataset subdirectories\n",
    "for directory in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAR_DATA = True\n",
    "\n",
    "# Directories to clean up\n",
    "DIRS = [\n",
    "    MODELS_DIR,\n",
    "    DATASET_DIR,\n",
    "    CHKPT_DIR,\n",
    "    TRAIN_DIR,\n",
    "    VAL_DIR,\n",
    "    TEST_DIR,\n",
    "    LOG_DIR,\n",
    "    CHKPT_DIR,\n",
    "]\n",
    "\n",
    "\n",
    "def remove_files_in_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        return\n",
    "\n",
    "    for root, dirs, files in os.walk(directory, topdown=False):\n",
    "        for name in files:\n",
    "            file_path = os.path.join(root, name)\n",
    "            try:\n",
    "                os.unlink(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "\n",
    "def cleanup_all_files(dirs=DIRS):\n",
    "    print(\n",
    "        \"WARNING: This will delete all files in the following directories and their subdirectories:\"\n",
    "    )\n",
    "    for dir in dirs:\n",
    "        if os.path.exists(dir):\n",
    "            total_files = sum([len(files) for _, _, files in os.walk(dir)])\n",
    "            print(f\"  - {dir} ({total_files} files)\")\n",
    "\n",
    "    confirmation = input(\"\\nType 'YES' to confirm deletion: \")\n",
    "\n",
    "    if confirmation == \"YES\":\n",
    "        for directory in dirs:\n",
    "            if os.path.exists(directory):\n",
    "                remove_files_in_directory(directory)\n",
    "                print(f\"Removed files from: {directory}\")\n",
    "        print(\"\\nAll files have been removed while preserving directory structure.\")\n",
    "    else:\n",
    "        print(\"\\nOperation cancelled.\")\n",
    "\n",
    "\n",
    "if CLEAR_DATA:\n",
    "    cleanup_all_files(DIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all files from data into notebook dataset directory\n",
    "!cp -r ../data/* {DATASET_DIR}\n",
    "\n",
    "# Get all the files in the dataset directory\n",
    "DATASET_FILES = glob.glob(os.path.join(DATASET_DIR, \"*\", \"*\", \"*\"))\n",
    "\n",
    "# Print the number of files in the dataset\n",
    "print(f\"Number of files in dataset: {len(DATASET_FILES)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python list to hold the dataset\n",
    "DATASET = []\n",
    "\n",
    "# Read each file and append to the dataset, add filename to the file data\n",
    "for f in DATASET_FILES:\n",
    "    with open(f, \"r\") as file:\n",
    "        f_c = file.read()\n",
    "    f_d = json.loads(f_c)\n",
    "    f_d[\"filename\"] = f\n",
    "    DATASET.append(f_d)\n",
    "\n",
    "print(f\"Number of files in dataset: {len(DATASET)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_matplotlib_backend(backend=\"inline\"):\n",
    "    \"\"\"\n",
    "    Switch matplotlib backend between 'inline' and 'notebook'\n",
    "    Args:\n",
    "        backend (str): Either 'inline' (static plots) or 'notebook' (interactive/animations)\n",
    "    \"\"\"\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    ipython.run_line_magic(\"matplotlib\", backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIFT_NAMES = {\"dC\": \"Dumbbell Curl\", \"bP\": \"Bench Press\", \"dF\": \"Dumbbell Fly\"}\n",
    "LIFT_CLASSES = {\n",
    "    \"p_f\": \"Perfect Form\",\n",
    "    \"l_i\": \"Lift Instability\",\n",
    "    \"p_m\": \"Partial Motion\",\n",
    "    \"o_a\": \"Off-Axis\",\n",
    "    \"s_w\": \"Swinging Weight\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lists(data):\n",
    "    time, aX, aY, aZ, gX, gY, gZ = [], [], [], [], [], [], []\n",
    "    for v in data:\n",
    "        time.append(v[\"t\"])\n",
    "        aX.append(v[\"aX\"])\n",
    "        aY.append(v[\"aY\"])\n",
    "        aZ.append(v[\"aZ\"])\n",
    "        gX.append(v[\"gX\"])\n",
    "        gY.append(v[\"gY\"])\n",
    "        gZ.append(v[\"gZ\"])\n",
    "    return time, aX, aY, aZ, gX, gY, gZ\n",
    "\n",
    "\n",
    "def plot_flat_dataset(data, show=True, save=False):\n",
    "    set_matplotlib_backend(\"inline\")\n",
    "    d = data[\"tSD\"]\n",
    "    lN = data[\"lN\"]\n",
    "    lC = data[\"lC\"]\n",
    "    time, aX, aY, aZ, gX, gY, gZ = create_lists(d)\n",
    "\n",
    "    # Create subplots for acceleration and gyroscope data\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "    # Acceleration subplot\n",
    "    axs[0].plot(time, aX, label=\"Ax\")\n",
    "    axs[0].plot(time, aY, label=\"Ay\")\n",
    "    axs[0].plot(time, aZ, label=\"Az\")\n",
    "    axs[0].set_title(\"3D Acceleration Over Time\")\n",
    "    axs[0].set_ylabel(\"Acceleration (m/s^2)\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Gyroscope subplot\n",
    "    axs[1].plot(time, gX, label=\"Gx\")\n",
    "    axs[1].plot(time, gY, label=\"Gy\")\n",
    "    axs[1].plot(time, gZ, label=\"Gz\")\n",
    "    axs[1].set_title(\"3D Gyroscope Over Time\")\n",
    "    axs[1].set_xlabel(\"Time (s)\")\n",
    "    axs[1].set_ylabel(\"Gyroscope (rad/s)\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Set Chart Title\n",
    "    fig.suptitle(f\"{LIFT_NAMES[lN]} - {LIFT_CLASSES[lC]}\", fontsize=16)\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    if save:\n",
    "        fig.savefig(f\"{PLOT_DIR}/{LIFT_NAMES[lN]}_{LIFT_CLASSES[lC]}_flat.png\")\n",
    "\n",
    "\n",
    "def plot_3d_trajectory(data, show=True, save=False):\n",
    "    set_matplotlib_backend(\"inline\")\n",
    "    d = data[\"tSD\"]\n",
    "    lN = data[\"lN\"]\n",
    "    lC = data[\"lC\"]\n",
    "    time, aX, aY, aZ, gX, gY, gZ = create_lists(d)\n",
    "\n",
    "    # Normalize time for color gradient\n",
    "    norm_time = (np.array(time) - min(time)) / (max(time) - min(time))\n",
    "\n",
    "    # Create 3D scatter plot for acceleration\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(121, projection=\"3d\")\n",
    "    scatter = ax.scatter(aX, aY, aZ, c=norm_time, cmap=\"viridis\", s=10)\n",
    "    ax.plot(aX, aY, aZ, color=\"gray\", alpha=0.5)  # Optional trajectory line\n",
    "    ax.set_title(\"3D Acceleration Trajectory\")\n",
    "    ax.set_xlabel(\"Ax (m/s^2)\")\n",
    "    ax.set_ylabel(\"Ay (m/s^2)\")\n",
    "    ax.set_zlabel(\"Az (m/s^2)\")\n",
    "    fig.colorbar(scatter, ax=ax, label=\"Time Gradient\")\n",
    "\n",
    "    # Create 3D scatter plot for gyroscope\n",
    "    ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "    scatter2 = ax2.scatter(gX, gY, gZ, c=norm_time, cmap=\"plasma\", s=10)\n",
    "    ax2.plot(gX, gY, gZ, color=\"gray\", alpha=0.5)  # Optional trajectory line\n",
    "    ax2.set_title(\"3D Gyroscope Trajectory\")\n",
    "    ax2.set_xlabel(\"Gx (rad/s)\")\n",
    "    ax2.set_ylabel(\"Gy (rad/s)\")\n",
    "    ax2.set_zlabel(\"Gz (rad/s)\")\n",
    "    fig.colorbar(scatter2, ax=ax2, label=\"Time Gradient\")\n",
    "\n",
    "    # Set Chart Title\n",
    "    fig.suptitle(f\"{LIFT_NAMES[lN]} - {LIFT_CLASSES[lC]}\", fontsize=16)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    if save:\n",
    "        fig.savefig(f\"{PLOT_DIR}/{LIFT_NAMES[lN]}_{LIFT_CLASSES[lC]}_3d.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_acceleration(time, aX, aY, aZ):\n",
    "    dt = np.diff(time, prepend=time[0])  # Time differences\n",
    "    # Integrate velocity\n",
    "    vX = np.cumsum(aX * dt)\n",
    "    vY = np.cumsum(aY * dt)\n",
    "    vZ = np.cumsum(aZ * dt)\n",
    "    # Integrate position\n",
    "    pX = np.cumsum(vX * dt)\n",
    "    pY = np.cumsum(vY * dt)\n",
    "    pZ = np.cumsum(vZ * dt)\n",
    "    return pX, pY, pZ\n",
    "\n",
    "\n",
    "def animate_3d_trajectory(data, filename, duration_factor=1):\n",
    "    set_matplotlib_backend(\"notebook\")\n",
    "    d = data[\"tSD\"]\n",
    "    lN = data[\"lN\"]\n",
    "    lC = data[\"lC\"]\n",
    "    time, aX, aY, aZ, gX, gY, gZ = create_lists(d)\n",
    "\n",
    "    # Integrate acceleration to find position\n",
    "    pX, pY, pZ = integrate_acceleration(time, aX, aY, aZ)\n",
    "\n",
    "    # Duration and timing\n",
    "    base_duration = 5  # Base duration in seconds\n",
    "    total_duration = base_duration * duration_factor\n",
    "    interval = (total_duration * 1000) / len(time)  # Interval in milliseconds\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 8))\n",
    "    ax = fig.add_subplot(131, projection=\"3d\")  # Acceleration\n",
    "    ax2 = fig.add_subplot(132, projection=\"3d\")  # Gyroscope\n",
    "    ax3 = fig.add_subplot(133, projection=\"3d\")  # Position\n",
    "\n",
    "    # Set axis limits dynamically to encompass all data\n",
    "    buffer = 0.1  # Add a 10% buffer for better visibility\n",
    "    ax.set_xlim([min(aX) - buffer, max(aX) + buffer])\n",
    "    ax.set_ylim([min(aY) - buffer, max(aY) + buffer])\n",
    "    ax.set_zlim([min(aZ) - buffer, max(aZ) + buffer])\n",
    "\n",
    "    ax2.set_xlim([min(gX) - buffer, max(gX) + buffer])\n",
    "    ax2.set_ylim([min(gY) - buffer, max(gY) + buffer])\n",
    "    ax2.set_zlim([min(gZ) - buffer, max(gZ) + buffer])\n",
    "\n",
    "    ax3.set_xlim([min(pX) - buffer, max(pX) + buffer])\n",
    "    ax3.set_ylim([min(pY) - buffer, max(pY) + buffer])\n",
    "    ax3.set_zlim([min(pZ) - buffer, max(pZ) + buffer])\n",
    "\n",
    "    # Styling for Acceleration\n",
    "    scatter_acc = ax.scatter([], [], [], c=[], cmap=\"viridis\", s=100)\n",
    "    (trajectory_acc,) = ax.plot([], [], [], color=\"blue\", alpha=0.6, linewidth=2)\n",
    "    ax.grid(True)\n",
    "    ax.set_title(\"3D Acceleration Animation\", fontsize=14)\n",
    "    ax.set_xlabel(\"Ax (m/s^2)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Ay (m/s^2)\", fontsize=12)\n",
    "    ax.set_zlabel(\"Az (m/s^2)\", fontsize=12)\n",
    "\n",
    "    # Styling for Gyroscope\n",
    "    scatter_gyro = ax2.scatter([], [], [], c=[], cmap=\"plasma\", s=100)\n",
    "    (trajectory_gyro,) = ax2.plot([], [], [], color=\"red\", alpha=0.6, linewidth=2)\n",
    "    ax2.grid(True)\n",
    "    ax2.set_title(\"3D Gyroscope Animation\", fontsize=14)\n",
    "    ax2.set_xlabel(\"Gx (rad/s)\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Gy (rad/s)\", fontsize=12)\n",
    "    ax2.set_zlabel(\"Gz (rad/s)\", fontsize=12)\n",
    "\n",
    "    # Styling for Position\n",
    "    scatter_pos = ax3.scatter([], [], [], c=[], cmap=\"cool\", s=100)\n",
    "    (trajectory_pos,) = ax3.plot([], [], [], color=\"green\", alpha=0.6, linewidth=2)\n",
    "    ax3.grid(True)\n",
    "    ax3.set_title(\"3D Position Animation (From Acceleration)\", fontsize=14)\n",
    "    ax3.set_xlabel(\"Px (m)\", fontsize=12)\n",
    "    ax3.set_ylabel(\"Py (m)\", fontsize=12)\n",
    "    ax3.set_zlabel(\"Pz (m)\", fontsize=12)\n",
    "\n",
    "    # Set Chart Title\n",
    "    fig.suptitle(f\"{LIFT_NAMES[lN]} - {LIFT_CLASSES[lC]}\", fontsize=16)\n",
    "\n",
    "    def init():\n",
    "        scatter_acc.set_offsets(np.empty((0, 3)))\n",
    "        scatter_gyro.set_offsets(np.empty((0, 3)))\n",
    "        scatter_pos.set_offsets(np.empty((0, 3)))\n",
    "        trajectory_acc.set_data([], [])\n",
    "        trajectory_acc.set_3d_properties([])\n",
    "        trajectory_gyro.set_data([], [])\n",
    "        trajectory_gyro.set_3d_properties([])\n",
    "        trajectory_pos.set_data([], [])\n",
    "        trajectory_pos.set_3d_properties([])\n",
    "        return (\n",
    "            scatter_acc,\n",
    "            scatter_gyro,\n",
    "            scatter_pos,\n",
    "            trajectory_acc,\n",
    "            trajectory_gyro,\n",
    "            trajectory_pos,\n",
    "        )\n",
    "\n",
    "    def update(frame):\n",
    "        # Acceleration\n",
    "        x_acc, y_acc, z_acc = aX[: frame + 1], aY[: frame + 1], aZ[: frame + 1]\n",
    "        scatter_acc._offsets3d = (x_acc, y_acc, z_acc)\n",
    "        trajectory_acc.set_data(x_acc, y_acc)\n",
    "        trajectory_acc.set_3d_properties(z_acc)\n",
    "\n",
    "        # Gyroscope\n",
    "        x_gyro, y_gyro, z_gyro = gX[: frame + 1], gY[: frame + 1], gZ[: frame + 1]\n",
    "        scatter_gyro._offsets3d = (x_gyro, y_gyro, z_gyro)\n",
    "        trajectory_gyro.set_data(x_gyro, y_gyro)\n",
    "        trajectory_gyro.set_3d_properties(z_gyro)\n",
    "\n",
    "        # Position\n",
    "        x_pos, y_pos, z_pos = pX[: frame + 1], pY[: frame + 1], pZ[: frame + 1]\n",
    "        scatter_pos._offsets3d = (x_pos, y_pos, z_pos)\n",
    "        trajectory_pos.set_data(x_pos, y_pos)\n",
    "        trajectory_pos.set_3d_properties(z_pos)\n",
    "\n",
    "        return (\n",
    "            scatter_acc,\n",
    "            scatter_gyro,\n",
    "            scatter_pos,\n",
    "            trajectory_acc,\n",
    "            trajectory_gyro,\n",
    "            trajectory_pos,\n",
    "        )\n",
    "\n",
    "    ani = FuncAnimation(\n",
    "        fig, update, frames=len(time), init_func=init, blit=False, interval=interval\n",
    "    )\n",
    "\n",
    "    # Save the animation\n",
    "    ani.save(filename, writer=\"ffmpeg\", fps=1000 // interval)\n",
    "    print(f\"Animation saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the directory already exists\n",
    "if not os.path.exists(ANIM_DIR):\n",
    "    try:\n",
    "        # Create the directory\n",
    "        os.makedirs(ANIM_DIR)\n",
    "        print(f\"Directory '{ANIM_DIR}' created successfully.\")\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that occur during directory creation\n",
    "        print(f\"Error creating directory '{ANIM_DIR}': {e}\")\n",
    "else:\n",
    "    print(f\"Directory '{ANIM_DIR}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16: Lift Instability\n",
    "# 28: Swinging Weight\n",
    "# 55: Off-Axis\n",
    "# 99: Partial Motion\n",
    "# 231: Perfect Form\n",
    "SAVE = False\n",
    "if SAVE:\n",
    "    for i in [16, 28, 55, 99, 231]:\n",
    "        data = DATASET[i]\n",
    "        EXT = \".mp4\"\n",
    "        anim_filename = f\"{ANIM_DIR}/{str(LIFT_NAMES[data['lN']]).replace('.json','')}_{str(LIFT_CLASSES[data['lC']])}_{EXT}\".replace(\n",
    "            \" \", \"_\"\n",
    "        )\n",
    "        animate_3d_trajectory(data, filename=anim_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video(\"anim/Dumbbell_Curl_Off-Axis_.mp4\", embed=True)\n",
    "# Video(\"anim/Dumbbell_Curl_Lift_Instability_.mp4\", embed=True)\n",
    "# Video(\"anim/Dumbbell_Curl_Partial_Motion_.mp4\", embed=True)\n",
    "# Video(\"anim/Dumbbell_Curl_Swinging_Weight_.mp4\", embed=True)\n",
    "Video(\"anim/Dumbbell_Curl_Perfect_Form_.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "SHOW = False\n",
    "if SAVE or SHOW:\n",
    "    for i in [16, 28, 55, 99, 231]:\n",
    "        data = DATASET[i]\n",
    "        plot_3d_trajectory(data, show=SHOW, save=SAVE)\n",
    "        plot_flat_dataset(data, show=SHOW, save=SAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split & Pre-Process Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PERCENTAGE = 10\n",
    "VALIDATION_PERCENTAGE = 20\n",
    "TRAIN_PERCENTAGE = 100 - (TEST_PERCENTAGE + VALIDATION_PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(LIFT_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_classes_in_dataset(dataset):\n",
    "    return set([d[\"lC\"] for d in dataset])\n",
    "\n",
    "\n",
    "def get_min_classes_covered(*args):\n",
    "    return min(len(check_classes_in_dataset(arg)) for arg in args)\n",
    "\n",
    "\n",
    "test_dataset = []\n",
    "validation_dataset = []\n",
    "train_dataset = []\n",
    "\n",
    "# Ensure that the test, validation, and train datasets contain all classes\n",
    "while (\n",
    "    get_min_classes_covered(test_dataset, validation_dataset, train_dataset)\n",
    "    != NUM_CLASSES\n",
    "):\n",
    "    shuffled_dataset = DATASET.copy()\n",
    "    random.shuffle(shuffled_dataset)\n",
    "\n",
    "    test_count = math.floor(len(shuffled_dataset) * TEST_PERCENTAGE / 100)\n",
    "    validation_count = math.floor(len(shuffled_dataset) * VALIDATION_PERCENTAGE / 100)\n",
    "    train_count = len(shuffled_dataset) - test_count - validation_count\n",
    "\n",
    "    test_dataset = shuffled_dataset[:test_count]\n",
    "    validation_dataset = shuffled_dataset[test_count : test_count + validation_count]\n",
    "    train_dataset = shuffled_dataset[test_count + validation_count :]\n",
    "\n",
    "print(\n",
    "    f\"Test Dataset: {len(test_dataset)} contains {len(check_classes_in_dataset(test_dataset))} classes.\"\n",
    ")\n",
    "print(\n",
    "    f\"Validation Dataset: {len(validation_dataset)} contains {len(check_classes_in_dataset(validation_dataset))} classes.\"\n",
    ")\n",
    "print(\n",
    "    f\"Train Dataset: {len(train_dataset)} contains {len(check_classes_in_dataset(train_dataset))} classes.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Size of Model\n",
    "VECTOR_X = len(DATASET[0][\"tSD\"][0]) - 1  # Remove the time column\n",
    "VECTOR_Y = 200  # Number of samples in the y-axis (downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_array(dataset):\n",
    "    # drops the time column\n",
    "    tSD = dataset[\"tSD\"]\n",
    "    label = dataset[\"lC\"]\n",
    "    return label, np.array(\n",
    "        [[ts[\"aX\"], ts[\"aY\"], ts[\"aZ\"], ts[\"gX\"], ts[\"gY\"], ts[\"gZ\"]] for ts in tSD]\n",
    "    )\n",
    "\n",
    "\n",
    "# Fill the head and tail of the dataset with aX - gZ of 0.\n",
    "def extend_dataset_length(dataset, target_length=1000):\n",
    "    if 0 < (n := target_length - len(dataset)):\n",
    "        head_padding = np.zeros((n // 2, VECTOR_X))\n",
    "        tail_padding = np.zeros((n - n // 2, VECTOR_X))\n",
    "        return np.concatenate([head_padding, dataset, tail_padding])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def downsample_dataset(dataset, target_length=VECTOR_Y, algo=\"avg\"):\n",
    "    # Algo Options: \"avg\", \"rand\"\n",
    "    if algo not in [\"avg\", \"rand\"]:\n",
    "        raise ValueError(f\"Invalid algorithm: {algo}. Use 'avg' or 'rand'.\")\n",
    "\n",
    "    n_time_steps, n_features = dataset.shape\n",
    "\n",
    "    # Handle case where target length is greater than dataset length\n",
    "    if target_length >= n_time_steps:\n",
    "        return extend_dataset_length(dataset, target_length)\n",
    "\n",
    "    window_size = n_time_steps // target_length\n",
    "\n",
    "    # Downsample using the chosen algorithm\n",
    "    if algo == \"avg\":\n",
    "        return np.array(\n",
    "            [\n",
    "                np.mean(\n",
    "                    dataset[i : i + window_size, :], axis=0\n",
    "                )  # Mean across each window\n",
    "                for i in range(0, n_time_steps - window_size + 1, window_size)\n",
    "            ]\n",
    "        )[\n",
    "            :target_length\n",
    "        ]  # Ensure we get exactly target_length samples\n",
    "\n",
    "    elif algo == \"rand\":\n",
    "        # For each window, select one random sample\n",
    "        downsampled = np.array(\n",
    "            [\n",
    "                dataset[\n",
    "                    i + np.random.randint(0, window_size), :\n",
    "                ]  # Random sample from window\n",
    "                for i in range(0, n_time_steps - window_size + 1, window_size)\n",
    "            ]\n",
    "        )[\n",
    "            :target_length\n",
    "        ]  # Ensure we get exactly target_length samples\n",
    "        return downsampled\n",
    "\n",
    "\n",
    "def normalize_sensor_data(data):\n",
    "    # Add samples dimension if input is single sample\n",
    "    if len(data.shape) == 2:\n",
    "        data = np.expand_dims(data, axis=0)\n",
    "\n",
    "    # Split accelerometer and gyroscope data\n",
    "    acc_data = data[..., :3]  # aX, aY, aZ\n",
    "    gyro_data = data[..., 3:]  # gX, gY, gZ\n",
    "\n",
    "    # Compute stats per sample (along timestep dimension)\n",
    "    acc_mean = np.mean(acc_data, axis=1, keepdims=True)\n",
    "    acc_std = np.std(acc_data, axis=1, keepdims=True)\n",
    "    gyro_mean = np.mean(gyro_data, axis=1, keepdims=True)\n",
    "    gyro_std = np.std(gyro_data, axis=1, keepdims=True)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    acc_std = np.where(acc_std == 0, 1e-7, acc_std)\n",
    "    gyro_std = np.where(gyro_std == 0, 1e-7, gyro_std)\n",
    "\n",
    "    # Normalize\n",
    "    acc_normalized = (acc_data - acc_mean) / acc_std\n",
    "    gyro_normalized = (gyro_data - gyro_mean) / gyro_std\n",
    "\n",
    "    normalized_data = np.concatenate([acc_normalized, gyro_normalized], axis=-1)\n",
    "\n",
    "    # Remove samples dimension if input was single sample\n",
    "    if normalized_data.shape[0] == 1:\n",
    "        normalized_data = normalized_data[0]\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "def signal_augmentation(data):\n",
    "    # Convert data to tensor and ensure float32 dtype\n",
    "    data = tf.cast(tf.convert_to_tensor(data), tf.float32)\n",
    "\n",
    "    # Define simpler augmentations with consistent dtypes\n",
    "    augs = [\n",
    "        (\n",
    "            \"noise\",\n",
    "            lambda data: data\n",
    "            + tf.cast(\n",
    "                tf.random.normal(data.shape, mean=0, stddev=0.03), dtype=tf.float32\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"rand_drop\",\n",
    "            lambda data: tf.where(\n",
    "                tf.random.uniform(data.shape) > 0.95, tf.zeros_like(data), data\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"mag_scale\",\n",
    "            lambda data: data\n",
    "            * tf.cast(tf.random.uniform([], minval=0.9, maxval=1.1), dtype=tf.float32),\n",
    "        ),\n",
    "        (\n",
    "            \"paired_inversion\",\n",
    "            lambda data: tf.where(\n",
    "                tf.random.uniform([]) > 0.5,\n",
    "                data\n",
    "                * tf.constant([-1.0, -1.0, 1.0, -1.0, -1.0, 1.0], dtype=tf.float32),\n",
    "                data,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Randomly select 1-2 augmentations\n",
    "    num_augs = tf.random.uniform([], minval=1, maxval=3, dtype=tf.int32)\n",
    "    indices = tf.random.shuffle(tf.range(len(augs)))[:num_augs]\n",
    "\n",
    "    augmented_data = data\n",
    "    for idx in indices:\n",
    "        _, aug_fn = augs[idx]\n",
    "        augmented_data = aug_fn(augmented_data)\n",
    "\n",
    "    return augmented_data\n",
    "\n",
    "\n",
    "def augment_dataset(data, min_copies=1, max_copies=3):\n",
    "    # Keep original\n",
    "    augmented_dataset = [data]\n",
    "\n",
    "    # Generate fixed number of copies\n",
    "    num_copies = np.random.randint(min_copies, max_copies + 1)\n",
    "\n",
    "    for _ in range(num_copies):\n",
    "        augmented_dataset.append(signal_augmentation(data))\n",
    "\n",
    "    return augmented_dataset\n",
    "\n",
    "\n",
    "def augment_dataset(data, min_copies=0, max_copies=5):\n",
    "    # Keep original\n",
    "    augmented_dataset = [data]\n",
    "    # Rand decide num of additional copies\n",
    "    num_copies = tf.random.uniform([], min_copies, max_copies, dtype=tf.int32)\n",
    "    for _ in range(num_copies):\n",
    "        augmented_dataset.append(signal_augmentation(data))\n",
    "    return augmented_dataset\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset, augment=False):\n",
    "    X = []  # Sensor Data\n",
    "    y = []  # Labels\n",
    "    for data in dataset:\n",
    "        label, data_array = convert_to_array(data)\n",
    "        data_array = extend_dataset_length(data_array, 1000)\n",
    "        data_array = downsample_dataset(data_array, VECTOR_Y, \"avg\")\n",
    "        if augment:\n",
    "            data_arrays = augment_dataset(data_array)\n",
    "            for arr in data_arrays:\n",
    "                arr = normalize_sensor_data(arr)\n",
    "                X.append(arr)\n",
    "                y.append(label)\n",
    "        else:\n",
    "            data_array = normalize_sensor_data(data_array)\n",
    "            X.append(data_array)\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normalization_comparison(original_sample, normalized_sample):\n",
    "    \"\"\"\n",
    "    Plot original vs normalized data for a single sample\n",
    "    \"\"\"\n",
    "    feature_names = [\"aX\", \"aY\", \"aZ\", \"gX\", \"gY\", \"gZ\"]\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "    # Plot original data\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        axes[0].plot(original_sample[:, i], label=feature)\n",
    "    axes[0].set_title(\"Original Data\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Plot normalized data\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        axes[1].plot(normalized_sample[:, i], label=feature)\n",
    "    axes[1].set_title(\"Normalized Data\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_NORMALIZATION = False\n",
    "if SHOW_NORMALIZATION:\n",
    "    # Visualize normalization for a few samples\n",
    "    for i in [16, 28, 55, 99, 231]:  # Show rep sample\n",
    "        print(\n",
    "            f\"\\nSample {i} - {LIFT_NAMES[DATASET[i]['lN']]} - {LIFT_CLASSES[DATASET[i]['lC']]}\"\n",
    "        )\n",
    "        label, original_data = convert_to_array(DATASET[i])\n",
    "        extended_data = extend_dataset_length(original_data, 1000)\n",
    "        downsampled_data = downsample_dataset(extended_data, VECTOR_Y, \"avg\")\n",
    "        normalized_data = normalize_sensor_data(downsampled_data)\n",
    "        plot_normalization_comparison(downsampled_data, normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_X, TEST_Y = preprocess_dataset(test_dataset, augment=False)\n",
    "VALIDATION_X, VALIDATION_Y = preprocess_dataset(validation_dataset, augment=False)\n",
    "TRAIN_X, TRAIN_Y = preprocess_dataset(train_dataset, augment=True)\n",
    "\n",
    "print(TEST_X.shape, TEST_Y.shape)\n",
    "print(VALIDATION_X.shape, VALIDATION_Y.shape)\n",
    "print(TRAIN_X.shape, TRAIN_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Label Output\n",
    "labels = sorted(set(TEST_Y).union(set(VALIDATION_Y)).union(set(TRAIN_Y)))\n",
    "labelToInt = {}\n",
    "currInt = 0\n",
    "for label in labels:\n",
    "    labelToInt[label] = currInt\n",
    "    currInt = currInt + 1\n",
    "intToLabel = {v: k for k, v in labelToInt.items()}\n",
    "print(intToLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding of Labels\n",
    "TEST_Y_cat = tf.keras.utils.to_categorical([labelToInt[y] for y in TEST_Y])\n",
    "VALIDATION_Y_cat = tf.keras.utils.to_categorical([labelToInt[y] for y in VALIDATION_Y])\n",
    "TRAIN_Y_cat = tf.keras.utils.to_categorical([labelToInt[y] for y in TRAIN_Y])\n",
    "\n",
    "# Define Batch Size\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 256\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "# Create tensorflow datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((TRAIN_X, TRAIN_Y_cat))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((VALIDATION_X, VALIDATION_Y_cat))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((TEST_X, TEST_Y_cat))\n",
    "\n",
    "train_ds = (\n",
    "    train_ds.cache().shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(AUTO)\n",
    ")\n",
    "val_ds = val_ds.cache().batch(BATCH_SIZE).prefetch(AUTO)\n",
    "test_ds = test_ds.cache().batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "\n",
    "for x, y in train_ds.take(1):\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "# The OUT_OF_RANGE error is OKAY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (VECTOR_Y, VECTOR_X)\n",
    "\n",
    "dropout_rate = 0.4\n",
    "\n",
    "\n",
    "def create_model(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.GRU(units=64, return_sequences=False)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        num_classes,\n",
    "        activation=\"softmax\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "    )(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_ds)\n",
    "\n",
    "# Define learning rate parameters\n",
    "initial_learning_rate = 0.001\n",
    "warmup_epochs = 50\n",
    "warmup_steps = warmup_epochs * steps_per_epoch\n",
    "first_decay_epochs = 500\n",
    "first_decay_steps = first_decay_epochs * steps_per_epoch\n",
    "\n",
    "# Cosine Decay with Warmup schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "    initial_learning_rate,\n",
    "    first_decay_steps=first_decay_steps,\n",
    "    t_mul=2,  # Each restart cycle will be 1.5x longer than the previous\n",
    "    m_mul=0.98,  # Each restart will have 0.95x the max learning rate of the previous\n",
    "    alpha=0.2,  # Minimum learning rate will be 10% of initial\n",
    ")\n",
    "\n",
    "# Update optimizer with new schedule\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=lr_schedule,\n",
    "    weight_decay=0.0005,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    ")\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "EPOCHS = 3000  # MAKE SURE THAT THE VAL_THRESHOLD IS MET\n",
    "\n",
    "checkpoint_filepath = os.path.join(CHKPT_DIR, \"cp-{epoch:04d}.keras\")\n",
    "os.makedirs(CHKPT_DIR, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "val_threshold = 0.95j\n",
    "\n",
    "\n",
    "class EarlyStopping(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get(\"val_accuracy\") > val_threshold:\n",
    "            print(f\"\\nReached {val_threshold} accuracy, stopping training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "early_stopping_callback = EarlyStopping()\n",
    "\n",
    "log_dir = os.path.join(LOG_DIR, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir={log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[checkpoint_callback, tensorboard_callback, early_stopping_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "model.export(f\"{SAVED_MODEL_FILENAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a non quantized model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(f\"{SAVED_MODEL_FILENAME}\")\n",
    "model_no_quant_tflite = converter.convert()\n",
    "\n",
    "with open(FLOAT_TFL_MODEL_FILENAME, \"wb\") as f:\n",
    "    f.write(model_no_quant_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize the model\n",
    "def representative_dataset():\n",
    "    for data in train_ds.take(10):\n",
    "        yield [data[0]]\n",
    "\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Enforce integer only quantization\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "\n",
    "# Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_dataset\n",
    "model_tflite = converter.convert()\n",
    "\n",
    "with open(QUANTIZED_TFL_MODEL_FILENAME, \"wb\") as f:\n",
    "    f.write(model_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model sizes\n",
    "def get_dir_size(dir_path):\n",
    "    total_size = 0\n",
    "    for dirpath, _, filenames in os.walk(dir_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "\n",
    "# Calculate sizes\n",
    "size_tf = get_dir_size(SAVED_MODEL_FILENAME)\n",
    "size_no_quant_tflite = os.path.getsize(FLOAT_TFL_MODEL_FILENAME)\n",
    "size_tflite = os.path.getsize(QUANTIZED_TFL_MODEL_FILENAME)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "size_comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"TensorFlow\", \"TensorFlow Lite\", \"TensorFlow Lite Quantized\"],\n",
    "        \"Size\": [\n",
    "            f\"{size_tf:,} bytes\",\n",
    "            f\"{size_no_quant_tflite:,} bytes\",\n",
    "            f\"{size_tflite:,} bytes\",\n",
    "        ],\n",
    "        \"Reduction\": [\n",
    "            \"\",\n",
    "            f\"(reduced by {size_tf - size_no_quant_tflite:,} bytes)\",\n",
    "            f\"(reduced by {size_no_quant_tflite - size_tflite:,} bytes)\",\n",
    "        ],\n",
    "    }\n",
    ").set_index(\"Model\")\n",
    "\n",
    "# Display comparison\n",
    "size_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
